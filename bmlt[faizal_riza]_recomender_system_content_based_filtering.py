# -*- coding: utf-8 -*-
"""BMLP[Faizal Riza] Recomender_System_Content_Based_Filtering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10tE-PPFneNDh_Isz0SVfHkjUEzJZtGL_

# Book Recommendation System

- Content-Based Collaborative Filtering using Title, Author, Publisher, Category as features

## About Dataset
Terdapat 278858 user memberikan 1149780 penilaian (explicit/implicit) terhadap 271379 buku
- user_id - id dari pengguna
- location - lokasi/alamat pengguna
- age - umur pengguna
- isbn - kode ISBN (International Standard Book Number) buku
- rating - rating dari buku
- book_title - judul buku
- book_author - penulis buku
- year_of_publication - tahun terbit buku
- publisher - penerbit buku
- img_s - gambar sampul buku (small)
- img_m - gambar sampul buku (medium)
- img_l - gambar sampul buku (large)
- Summary - ringkasan/sinopsis buku
- Language - bahasa yang digunakan buku
- Category - kategori buku
- city - kota pengguna
- state - negara bagian penguna
- country - negara pengguna

## Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install opendatasets

import os
import re
import nltk
import requests
import warnings
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import opendatasets as od

from nltk.corpus import stopwords
nltk.download("stopwords")

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity

from PIL import Image

"""## Load and Check Dataset"""

od.download('https://www.kaggle.com/datasets/ruchi798/bookcrossing-dataset')

books = pd.read_csv('/content/bookcrossing-dataset/Books Data with Category Language and Summary/Preprocessed_data.csv')
books.head(2)

books.info()

print(sorted(books.rating.unique()))
print()
print(books.rating.value_counts())

books.isnull().sum()

print(books.Category.unique())
print()
print(books.Category.value_counts().index)

# Title, Author, Publisher, Category as features
books.publisher.value_counts()

"""## Preprocessing"""

df = books.copy()
df.dropna(inplace=True, how='any', axis=0)
df.reset_index(drop=True, inplace=True)
df.drop(columns = ['Unnamed: 0','location','isbn',
                   'img_s','img_m', 'img_l', 'city','age',
                   'state','Language','country',
                   'year_of_publication', 'Summary'],axis=1,inplace = True) #kolom yang didrop tidak akan dipakai
df.drop(index=df[df.Category == '9'].index, inplace=True)
df.drop(index=df[df.rating == 0].index, inplace=True)
df.Category = df.Category.apply(lambda x: re.sub('[\W_]+', ' ', x).strip())
df.head()

df.info()

df.isnull().sum()

# df.Category.value_counts()
i = 1
for idx, name in enumerate(df['Category'].value_counts().index.tolist()):
    if(i==25): break
    print(i)
    print('Name :', name)
    print('Counts :', df['Category'].value_counts()[idx])
    print('---'*8)
    i+=1

cat_list = df.Category.value_counts().index.tolist()
print(cat_list[5:20])

df_fil = df[df.Category.isin(cat_list[5:20])]
df_fil.info()

df_fil.Category.nunique()

prep = df_fil.copy()
prep.sort_values('book_title')

prep = prep.drop_duplicates('book_title')
prep.info()
print()
prep.head(4)

prep['Category'] = prep['Category'].str.replace(' ', '_')
prep.head(10)

book_title = prep['book_title'].tolist()
book_cat = prep['Category'].tolist()
book_pub = prep['publisher'].tolist()
book_author = prep['book_author'].tolist()

print(len(book_title))
print(len(book_cat))
print(len(book_pub))
print(len(book_author))

book_new = pd.DataFrame({
    'title': book_title,
    'author': book_author,
    'category': book_cat,
    'publisher': book_pub
})
book_new

"""### TF-IDF"""

from sklearn.feature_extraction.text import TfidfVectorizer
#Init Tfidf
tf = TfidfVectorizer()

# Melakukan perhitung idf pada data category
tf.fit(book_new['category'])

# Mapping array dari fitur index integer ke fitur nama
tf.get_feature_names_out()

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tf.fit_transform(book_new['category'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

# Membuat dataframe untuk melihat tf-idf matrix
# Kolom diisi dengan category book
# Baris diisi dengan nama book

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=book_new.title
).sample(5, axis=1).sample(10, axis=0)

"""### Cosine Similarity"""

from sklearn.metrics.pairwise import cosine_similarity

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa nama resto
cosine_sim_df = pd.DataFrame(cosine_sim, index=book_new['title'], columns=book_new['title'])
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap resto
cosine_sim_df.sample(5, axis=1).sample(3, axis=0)

"""### Mendapatkan rekomendasi"""

def book_recommendation(nama_buku, similarity_data=cosine_sim_df, items=book_new[['title', 'category']], k=5):
    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,nama_buku].to_numpy().argpartition(
        range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop nama_resto agar nama resto yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(nama_buku, errors='ignore')
    df = pd.DataFrame(closest).merge(items)
    df.drop_duplicates(keep='first', subset="title", inplace=True)
    return df.head(k)

book_new.head()

book_new[book_new['title'].eq("McDonald's: Behind the Arches")]

book_recommendation("McDonald's: Behind the Arches", k=10)